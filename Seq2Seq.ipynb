{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZdhHtdiAZLpckHlhsyazN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/100jy/dacon_ts_forecasting/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyusmuWLCLOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d417ef-aefe-4ad2-e1fb-696e13f6ec10"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD6aITxrCN2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe87c38-2bee-4500-85d3-268232470193"
      },
      "source": [
        "!pip install torchcontrib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchcontrib\n",
            "  Downloading https://files.pythonhosted.org/packages/72/36/45d475035ab35353911e72a03c1c1210eba63b71e5a6917a9e78a046aa10/torchcontrib-0.0.2.tar.gz\n",
            "Building wheels for collected packages: torchcontrib\n",
            "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-cp36-none-any.whl size=7531 sha256=b4cb32806eb59c4f0c5185868752bf63e895d1158dec1e3a09eff0a40e992233\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/06/7b/a5f5920bbf4f12a2c927e438fac17d4cd9560f8336b00e9a99\n",
            "Successfully built torchcontrib\n",
            "Installing collected packages: torchcontrib\n",
            "Successfully installed torchcontrib-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b1-xV5YCMtF"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.utils.data.dataloader import DataLoader\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import torch.optim.adam\r\n",
        "from torchcontrib.optim import SWA\r\n",
        "import datetime\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFcUnZhJUWn"
      },
      "source": [
        "# feature 생성 및 preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJqiSxB-CRRE"
      },
      "source": [
        "train = pd.read_csv(\"./drive/MyDrive/데이콘/train.csv\", encoding = 'euc-kr')\r\n",
        "\r\n",
        "# 시간 관련 변수들\r\n",
        "train['DateTime'] = pd.to_datetime(train.DateTime)\r\n",
        "#일자\r\n",
        "train['Date'] = train.DateTime.dt.date\r\n",
        "\r\n",
        "# 요일 혹은 분기정보\r\n",
        "train['DayOfWeek'] = (train.DateTime.dt.weekday)/6\r\n",
        "train['DayOfMon'] = ((train.DateTime).dt.day)/31\r\n",
        "train['Quarter'] = ((train.DateTime).dt.quarter)/4\r\n",
        "\r\n",
        "train['Year'] = ((train.DateTime.dt.year) -2019)\r\n",
        "train['Days'] = (train.DateTime.max() - train.DateTime).dt.days + 1\r\n",
        "\r\n",
        "\r\n",
        "left = train.iloc[:,:5].groupby(train['Date']).sum().reset_index()\r\n",
        "right = train.iloc[:,5:].groupby(train['Date']).mean().reset_index()\r\n",
        "train  = pd.merge(left, right, on='Date')\r\n",
        "\r\n",
        "def log_trans(x):\r\n",
        "  return np.log(1+x)\r\n",
        "\r\n",
        "train['Days'] = log_trans(train['Days'])\r\n",
        "\r\n",
        "\r\n",
        "# ts feature 생성 \r\n",
        "for target in ['사용자', '세션', '신규방문자', '페이지뷰']:\r\n",
        "    train[f'{target}CumSum'] = train[target].cumsum()\r\n",
        "    # log하고 rolling mean\r\n",
        "    train[target] = log_trans(train[target])\r\n",
        "    \r\n",
        "    for k in [3,7,14,21]:\r\n",
        "        train[f'{target}RollingMean{k}'] =  (train[target].rolling(k).mean())\r\n",
        "\r\n",
        "    train[f'{target}RollingStd21'] =  (train[target].rolling(21).std().round(0))\r\n",
        "    train[f'{target}DaysSince10000'] = (train[f'{target}CumSum'] > 10000) * 1\r\n",
        "    train[f'{target}DaysSince100000'] = (train[f'{target}CumSum'] > 100000) * 1\r\n",
        "\r\n",
        "    train[f'{target}RollingMeanDiff2w'] = train[f'{target}RollingMean7'] / (train[f'{target}RollingMean14'] + 1) - 1\r\n",
        "    train[f'{target}RollingMeanDiff3w'] = train[f'{target}RollingMean7'] / (train[f'{target}RollingMean21'] + 1) - 1\r\n",
        "\r\n",
        "\r\n",
        "    train[f'{target}CumSum'] = log_trans(train[target].cumsum())\r\n",
        "    \r\n",
        "    \r\n",
        "train_df = train.dropna()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2N_Eyr9JZOu"
      },
      "source": [
        "# Dataset 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz6OfvUNCSaF"
      },
      "source": [
        "MAX_LENGTH = 180\r\n",
        "class DatasetWindows(Dataset):\r\n",
        "  def __init__(self, df=train_df):\r\n",
        "    self.df = df.iloc[:,1:]\r\n",
        "    self.max_len = len(df)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return 1\r\n",
        "    \r\n",
        "  def __getitem__(self, idx):\r\n",
        "\r\n",
        "    #choose length(90, 200)\r\n",
        "    seq_length = np.random.randint(90,181)\r\n",
        "    \r\n",
        "    # choose start point\r\n",
        "    x_point = np.random.randint(0, self.max_len - seq_length)\r\n",
        "    # half as x, half as y\r\n",
        "    y_point = x_point + seq_length//2\r\n",
        "\r\n",
        "    x = torch.tensor(self.df.iloc[x_point:y_point, :].values, dtype=torch.float32).cuda()\r\n",
        "    y = torch.tensor(self.df.iloc[y_point:x_point+seq_length, :4].values, dtype=torch.float32).cuda()\r\n",
        "    # L x 49\r\n",
        "    return x, y"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIWypPoAJbi-"
      },
      "source": [
        "# Model 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33HIkz2wCZ40"
      },
      "source": [
        "class Embedding(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(Embedding, self).__init__()\r\n",
        "     \r\n",
        "        self.embedding = nn.Sequential(nn.Linear(input_size, hidden_size))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        embedded = self.embedding(input)\r\n",
        "        return embedded\r\n",
        "\r\n",
        "class EncoderRNN(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(EncoderRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        self.embedding = Embedding(input_size, hidden_size)\r\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\r\n",
        "\r\n",
        "    def forward(self, input, hidden):\r\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\r\n",
        "        output = embedded\r\n",
        "        output, hidden = self.gru(output, hidden)\r\n",
        "        return output, hidden\r\n",
        "\r\n",
        "    def initHidden(self):\r\n",
        "        return torch.zeros(1, 1, self.hidden_size).cuda()\r\n",
        "\r\n",
        "\r\n",
        "class AttnDecoderRNN(nn.Module):\r\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\r\n",
        "        super(AttnDecoderRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.dropout_p = dropout_p\r\n",
        "        self.max_length = max_length\r\n",
        "\r\n",
        "        self.embedding = Embedding(self.output_size, self.hidden_size)\r\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\r\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\r\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\r\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\r\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\r\n",
        "\r\n",
        "    def forward(self, input, hidden, encoder_outputs):\r\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\r\n",
        "        embedded = self.dropout(embedded)\r\n",
        "\r\n",
        "        attn_weights = F.softmax(\r\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\r\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\r\n",
        "                                 encoder_outputs.unsqueeze(0))\r\n",
        "\r\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\r\n",
        "        output = self.attn_combine(output).unsqueeze(0)\r\n",
        "\r\n",
        "        output = F.relu(output)\r\n",
        "        output, hidden = self.gru(output, hidden)\r\n",
        "        output = self.out(output[0])\r\n",
        "        return output, hidden, attn_weights\r\n",
        "\r\n",
        "    def initHidden(self):\r\n",
        "        return torch.zeros(1, 1, self.hidden_size).cuda()\r\n",
        "\r\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPCaZ0k4CiRl"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\r\n",
        "    encoder_hidden = encoder.initHidden()\r\n",
        "\r\n",
        "    encoder_optimizer.zero_grad()\r\n",
        "    decoder_optimizer.zero_grad()\r\n",
        "\r\n",
        "    input_length = input_tensor.size(0)\r\n",
        "    target_length = target_tensor.size(0)\r\n",
        "\r\n",
        "    # L x 32\r\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size).cuda()\r\n",
        "\r\n",
        "    loss = 0\r\n",
        "\r\n",
        "    for ei in range(input_length):\r\n",
        "        encoder_output, encoder_hidden = encoder(\r\n",
        "            input_tensor[ei], encoder_hidden)\r\n",
        "        encoder_outputs[ei] = encoder_output\r\n",
        "\r\n",
        "    # input last day\r\n",
        "    decoder_input = input_tensor[-1,:4].unsqueeze(0)\r\n",
        "    # last hidden state as initial\r\n",
        "    decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "\r\n",
        "    for di in range(target_length):\r\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "        decoder_input = decoder_output.squeeze().detach()\r\n",
        "        loss += criterion(decoder_output, target_tensor[di])\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    encoder_optimizer.step()\r\n",
        "    decoder_optimizer.step()\r\n",
        "\r\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8HpwPPkn3mp"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def asMinutes(s):\r\n",
        "    m = math.floor(s / 60)\r\n",
        "    s -= m * 60\r\n",
        "    return '%dm %ds' % (m, s)\r\n",
        "\r\n",
        "\r\n",
        "def timeSince(since, percent):\r\n",
        "    now = time.time()\r\n",
        "    s = now - since\r\n",
        "    es = s / (percent)\r\n",
        "    rs = es - s\r\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\r\n",
        "\r\n",
        "\r\n",
        "def showPlot(points):\r\n",
        "    plt.figure()\r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    # this locator puts ticks at regular intervals\r\n",
        "    loc = ticker.MultipleLocator(base=0.2)\r\n",
        "    ax.yaxis.set_major_locator(loc)\r\n",
        "    plt.plot(points)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijrxK0zmwEpX"
      },
      "source": [
        "def evaluate(encoder, decoder, input_seq, max_length=MAX_LENGTH):\r\n",
        "    with torch.no_grad():\r\n",
        "        input_tensor = torch.tensor(input_seq, dtype=torch.float32).cuda()\r\n",
        "        input_length = input_tensor.size()[0]\r\n",
        "        encoder_hidden = encoder.initHidden()\r\n",
        "\r\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size).cuda()\r\n",
        "\r\n",
        "        for ei in range(input_length):\r\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\r\n",
        "                                                     encoder_hidden)\r\n",
        "            encoder_outputs[ei] = encoder_output\r\n",
        "\r\n",
        "        decoder_input = input_tensor[-1,:4].unsqueeze(0)\r\n",
        "\r\n",
        "        decoder_hidden = encoder_hidden\r\n",
        "\r\n",
        "        decoded =  torch.zeros(1, 4).cuda()\r\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\r\n",
        "\r\n",
        "        for di in range(max_length):\r\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\r\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\r\n",
        "            decoder_attentions[di] = decoder_attention.data\r\n",
        "            decoded = torch.cat([decoded, decoder_output],dim=0)\r\n",
        "            decoder_input = decoder_output.squeeze().detach()\r\n",
        "        return decoded[1:]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZiDDfpCzXdl"
      },
      "source": [
        "def inverse_log(x):\r\n",
        "    # 32bit 사용시 단위문제 발생..\r\n",
        "    return (np.exp(x)-1).astype(np.int64)\r\n",
        "\r\n",
        "def make_val_plot(encoder, attn_decoder, inp, val_df, only_loss=True):\r\n",
        "  pred = evaluate(encoder, attn_decoder, inp.iloc[-120:, :].values)[:61,:]\r\n",
        "  # 120일치중 61일까지\r\n",
        "  pred = inverse_log(pred.detach().cpu().numpy())\r\n",
        "  label =inverse_log(val_df)\r\n",
        "\r\n",
        "  def dacon_rmse(true, pred):  \r\n",
        "    w0 = 1095.214646\r\n",
        "    w1 = 1086.728535\r\n",
        "    w2 = 268.070707\r\n",
        "    w3 = 24236.194444\r\n",
        "\r\n",
        "    score = (np.sqrt(np.mean(np.square(true[:,0] - pred[:,0]))) / w0 + \r\n",
        "              np.sqrt(np.mean(np.square(true[:,1] - pred[:,1]))) / w1 + \r\n",
        "              np.sqrt(np.mean(np.square(true[:,2] - pred[:,2]))) / w2 + \r\n",
        "              np.sqrt(np.mean(np.square(true[:,3] - pred[:,3]))) / w3  )\r\n",
        "    return score\r\n",
        "\r\n",
        "  if only_loss:\r\n",
        "    return dacon_rmse(label.iloc[:,:4].values, pred)\r\n",
        "\r\n",
        "  for idx, key in enumerate(val_df.columns):\r\n",
        "    plt.plot(figsize=(20,10))\r\n",
        "    plt.plot(label.index,pred[:,idx])\r\n",
        "    plt.plot(label[key])\r\n",
        "    plt.legend(['predict', 'label'])\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "  loss = dacon_rmse(label.iloc[:,:4].values, pred)\r\n",
        "  print('RMSE : ' + str(loss))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCMRHaPkdnyc"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, dataset, print_every=100, plot_every=100, learning_rate=0.01, val=True):\r\n",
        "    start = time.time()\r\n",
        "    plot_losses = []\r\n",
        "    print_loss_total = 0  # Reset every print_every\r\n",
        "    plot_loss_total = 0  # Reset every plot_every\r\n",
        "\r\n",
        "    adam_1 =  torch.optim.Adam(encoder.parameters(), lr=learning_rate)\r\n",
        "    adam_2 =  torch.optim.Adam(decoder.parameters(), lr=learning_rate)\r\n",
        " \r\n",
        "    encoder_optimizer = SWA(adam_1, swa_start=10, swa_freq=5, swa_lr=learning_rate/2)\r\n",
        "    decoder_optimizer = SWA(adam_2, swa_start=10, swa_freq=5, swa_lr=learning_rate/2)\r\n",
        "    criterion = nn.MSELoss()\r\n",
        "\r\n",
        "    inp = train_df.iloc[:-61,1:]\r\n",
        "    label = train_df.iloc[-61:,1:5]\r\n",
        "    best_val = float('inf')\r\n",
        "\r\n",
        "    for iter in range(1, n_iters + 1):\r\n",
        "        input_tensor, target_tensor = dataset.__getitem__(0)\r\n",
        "\r\n",
        "        loss = train(input_tensor, target_tensor, encoder,\r\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\r\n",
        "        print_loss_total += loss\r\n",
        "        plot_loss_total += loss\r\n",
        "\r\n",
        "        if iter % print_every == 0:\r\n",
        "            print_loss_avg = print_loss_total / print_every\r\n",
        "            print_loss_total = 0\r\n",
        "            if val:\r\n",
        "              val_loss =  make_val_plot(encoder, decoder, inp, label)\r\n",
        "              print('%s (%d %d%%) (%.4f %.4f)' % (timeSince(start, iter / n_iters),\r\n",
        "                                          iter, iter / n_iters * 100, print_loss_avg,val_loss))\r\n",
        "            else:\r\n",
        "              print('%s (%d %d%%) (%.4f)' % (timeSince(start, iter / n_iters),\r\n",
        "                                          iter, iter / n_iters * 100))\r\n",
        "            \r\n",
        "            if val_loss < best_val:\r\n",
        "              best = (encoder.state_dict() ,decoder.state_dict())\r\n",
        "\r\n",
        "        if iter % plot_every == 0:\r\n",
        "            plot_loss_avg = plot_loss_total / plot_every\r\n",
        "            plot_losses.append(plot_loss_avg)\r\n",
        "            plot_loss_total = 0\r\n",
        "    showPlot(plot_losses)\r\n",
        "\r\n",
        "    #다끝나면 구글 드라이브에 저장\r\n",
        "    torch.save(best[0], './drive/MyDrive/데이콘/encoder.ckpt')\r\n",
        "    torch.save(best[1],  './drive/MyDrive/데이콘/decoder.ckpt')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gIQWSSDp_Yg",
        "outputId": "04e3997f-68d0-4ee4-961f-a94670bf6c1e"
      },
      "source": [
        "input_size = 49\r\n",
        "hidden_size = 256\r\n",
        "output_size = 4\r\n",
        "dropout_p = 0.8\r\n",
        "dataset = DatasetWindows(train_df.iloc[:-61,:])\r\n",
        "\r\n",
        "encoder = EncoderRNN(input_size, hidden_size).cuda()\r\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_size, dropout_p=dropout_p).cuda()\r\n",
        "\r\n",
        "trainIters(encoder, attn_decoder, 10000, dataset, learning_rate=0.001, print_every=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2m 17s (- 43m 37s) (500 5%) (1.5985 7.0720)\n",
            "4m 33s (- 41m 3s) (1000 10%) (0.4640 4.1117)\n",
            "6m 52s (- 38m 57s) (1500 15%) (0.4070 3.8042)\n",
            "9m 8s (- 36m 34s) (2000 20%) (0.3457 3.8944)\n",
            "11m 28s (- 34m 26s) (2500 25%) (0.3379 4.0281)\n",
            "13m 47s (- 32m 10s) (3000 30%) (0.3184 3.6857)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRNByaBphAuc"
      },
      "source": [
        "input_size = 49\r\n",
        "hidden_size = 256\r\n",
        "output_size = 4\r\n",
        "dropout_p = 0.1\r\n",
        "dataset = DatasetWindows(train_df)\r\n",
        "\r\n",
        "encoder = EncoderRNN(input_size, hidden_size).cuda()\r\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_size, dropout_p=dropout_p).cuda()\r\n",
        "\r\n",
        "trainIters(encoder, attn_decoder, 3000, dataset, print_every=500, learning_rate=0.001,val=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}